# Frage
Wozu dient die „Forward Propagation“ in künstlichen Neuronalen Netzen? Wozu die„Backpropagation“? Warum braucht man das eine für das andere? (Gerne können Sie Ihre Antwort mit Formeln verdeutlichen.)

# Antwort
Die [[Forward Propagation]] (Vorwärtspropagation) und [[Backpropagation]] (Rückwärtspropagation) sind zwei entscheidende Schritte im Training von künstlichen neuronalen Netzen, die zur Umsetzung von maschinellem Lernen verwendet werden.

### Forward Propagation
Stell dir vor, du hast ein neuronales Netzwerk, das aus vielen miteinander verbundenen Neuronen besteht, ähnlich wie das menschliche Gehirn. 
Beim Start einer Vorwärtspropagation wird eine Eingabe (z.B. ein Bild) in das Netzwerk gegeben. Diese Eingabe durchläuft die Neuronen und die Verbindungen zwischen ihnen. Jedes Neuron berechnet eine gewichtete Summe der Eingabe und wendet dann eine Aktivierungsfunktion auf diese Summe an. Das Ergebnis wird an die nächsten Neuronen weitergegeben, bis die Ausgabe des Netzwerks erzeugt wird. 
Das Ziel der Vorwärtspropagation ist es, eine Vorhersage zu generieren, wie das Netzwerk auf eine bestimmte Eingabe reagiert.

### Backpropagation
Sobald das neuronale Netzwerk eine Vorhersage gemacht hat, vergleicht man diese Vorhersage mit dem **tatsächlichen erwarteten Ergebnis** (dem sogenannten Label). 
Hier kommt die Backpropagation ins Spiel. Die Backpropagation ist der Prozess, bei dem **der Fehler zwischen der Vorhersage und dem erwarteten Ergebnis** berechnet wird. 
Diese Fehlerinformation wird dann **rückwärts** durch das Netzwerk propagiert, indem man die Gradienten der Fehlerfunktion bezüglich der Gewichte und Biaswerte berechnet. Diese Gradienten zeigen an, wie stark und in welche Richtung die Gewichte und Biaswerte angepasst werden müssen, um den Fehler zu verringern.

### Warum braucht man das eine für das andere?
Die [[Backpropagation]] benötigt die [[Forward Propagation]], weil sie **den Fehler zwischen der Vorhersage und dem erwarteten Ergebnis** berechnet, der während der Vorwärtspropagation entstanden ist. Ohne die Vorwärtspropagation gäbe es keinen Vorhersagewert, mit dem man den Fehler vergleichen könnte.

Die [[Forward Propagation]] benötigt wiederum die [[Backpropagation]], um die **Gewichte und Biaswerte** des Netzwerks anzupassen. Wenn das Netzwerk Fehler in seinen Vorhersagen macht, müssen die Gewichte und Biaswerte so geändert werden, dass diese Fehler **minimiert** werden. Die Gradienten, die während der Backpropagation berechnet werden, zeigen genau an, wie diese Anpassungen vorgenommen werden müssen.

### Zusammengefasst:
Die Vorwärtspropagation erzeugt Vorhersagen, die dann verwendet werden, um den Fehler zu berechnen, den die Backpropagation nutzt, um die Gewichte und Biaswerte zu optimieren und das Netzwerk zu trainieren.

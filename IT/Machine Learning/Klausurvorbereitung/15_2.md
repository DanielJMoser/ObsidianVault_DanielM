# Frage
Angenommen, Sie trainieren ein Modell der linearen Regression. Welche beiden Methoden zum Finden der optimalen Parameter kennen Sie? 
Nennen Sie jeweils (mindestens) einen Vorteil und einen Nachteil der jeweiligen Methode.

# Antwort
Bei der linearen Regression geht es darum, eine lineare Beziehung zwischen Eingangsvariablen (Features) und einer Ausgangsvariable zu finden. Zwei gängige Methoden zur Ermittlung der optimalen Parameter sind die **Methode der kleinsten Quadrate** (Least Squares Method) und die **Gradientenabstiegs-Methode** (Gradient Descent).

### Methode der kleinsten Quadrate:
- **Vorteil:** Diese Methode hat eine geschlossene Formel, die direkt die optimalen Parameter berechnet. Sie ist _einfach zu implementieren_ und effizient für _kleinere_ Datensätze.
- **Nachteil:** Bei großen Datensätzen kann die Berechnung der _inversen Matrix_ (die in der Formel vorkommt) _rechenintensiv_ und _speicherintensiv_ sein. Zudem ist die Methode empfindlich gegenüber Ausreißern, da sie versucht, den quadratischen Fehler insgesamt zu minimieren.

### Gradient Descent
- **Vorteil:** Diese Methode funktioniert gut für _große_ Datensätze, da sie die Daten nicht komplett in den Speicher laden muss. Sie kann auch bei _nicht-linearen_ Modellen angewendet werden. Zudem kann sie mit verschiedenen _Fehlerfunktionen_ erweitert werden.
- **Nachteil:** Die Gradientenabstiegs-Methode erfordert _die Auswahl einer Lernrate_, die den Konvergenzprozess beeinflusst. Eine zu hohe Lernrate kann zu _Divergenz_ führen, während eine zu niedrige Lernrate den Konvergenzprozess _verlangsamen_ kann. Die Methode kann in lokalen Minima steckenbleiben, wenn die Fehleroberfläche unregelmäßig ist.

Beide Methoden haben ihre Vor- und Nachteile, und die Wahl zwischen ihnen hängt von der **Größe des Datensatzes, der Problemkomplexität und den vorhandenen Ressourcen** ab. Die Methode der kleinsten Quadrate ist eine analytische Methode, während die Gradientenabstiegs-Methode ein iterativer Optimierungsansatz ist.

[[15_3]]